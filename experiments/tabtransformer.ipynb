{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import torch\n",
    "\n",
    "import lightning as L\n",
    "\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from torch_geometric.nn import conv\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.typing import EdgeType, NodeType\n",
    "\n",
    "import torch_frame\n",
    "from torch_frame.data import StatType\n",
    "\n",
    "from db_transformer.nn.embedder import TabTransformerEmbedder\n",
    "from db_transformer.data.ctu_dataset import CTUDataset\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        target_table: str,\n",
    "        table_col_stats: Dict[NodeType, Dict[str, Dict[StatType, Any]]],\n",
    "        table_col_names_dict: Dict[torch_frame.stype, List[str]],\n",
    "        edge_types: List[EdgeType],\n",
    "        embed_dim: int,\n",
    "        num_embedder_layers: int,\n",
    "        num_embedder_heads: int,\n",
    "        embedder_attn_dropout: float,\n",
    "        out_dim: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.target_table = target_table\n",
    "\n",
    "        self.table_embedder = TabTransformerEmbedder(\n",
    "            table_col_stats=table_col_stats,\n",
    "            table_col_names_dict=table_col_names_dict,\n",
    "            embed_dim=embed_dim,\n",
    "            num_transformer_layers=num_embedder_layers,\n",
    "            num_transformer_heads=num_embedder_heads,\n",
    "            attn_dropout=embedder_attn_dropout,\n",
    "        )\n",
    "\n",
    "        # TODO: This can be also a cross-attention layer\n",
    "        convs = {\n",
    "            edge_type: conv.SAGEConv(in_channels=embed_dim, out_channels=embed_dim)\n",
    "            for edge_type in edge_types\n",
    "        }\n",
    "        self.conv = conv.HeteroConv(convs)\n",
    "\n",
    "        self.out_lin = torch.nn.Linear(embed_dim, out_dim)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        tf_dict: Dict[str, torch_frame.TensorFrame],\n",
    "        edge_dict: Dict[str, torch.Tensor],\n",
    "    ):\n",
    "        x_dict = self.table_embedder(tf_dict)\n",
    "        x_dict = self.conv(x_dict, edge_dict)\n",
    "\n",
    "        x_target = x_dict[self.target_table]\n",
    "\n",
    "        x_target = self.out_lin(x_target)\n",
    "        return torch.softmax(x_target, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningModel(L.LightningModule):\n",
    "    def __init__(self, model: TabTransformerEmbedder, target_table: str, lr: float) -> None:\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.target_table = target_table\n",
    "        self.lr = lr\n",
    "        self.loss_module = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, data: HeteroData):\n",
    "        out = self.model(data.collect(\"tf\"), data.collect(\"edge_index\"))\n",
    "\n",
    "        target = data[self.target_table].y\n",
    "        loss = self.loss_module(out, target)\n",
    "        acc = (out.argmax(dim=-1) == target).sum().float() / target.shape[0]\n",
    "        return loss, acc\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        loss, acc = self.forward(batch)\n",
    "        batch_size = batch[self.target_table].y.shape[0]\n",
    "        self.log(\"train_loss\", loss, batch_size=batch_size, prog_bar=True)\n",
    "        self.log(\"train_acc\", acc, batch_size=batch_size, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        _, acc = self.forward(batch)\n",
    "\n",
    "        self.log(\"val_acc\", acc, batch_size=1, prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch):\n",
    "        _, acc = self.forward(batch)\n",
    "\n",
    "        self.log(\"test_acc\", acc, batch_size=1, prog_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table content has stypes: {'paper_id': <stype.categorical: 'categorical'>, 'word_cited_id': <stype.categorical: 'categorical'>}\n",
      "Table cites has stypes: {'cited_paper_id': <stype.text_embedded: 'text_embedded'>, 'citing_paper_id': <stype.text_embedded: 'text_embedded'>}\n",
      "Table paper has stypes: {'paper_id': <stype.text_embedded: 'text_embedded'>, 'class_label': <stype.categorical: 'categorical'>}\n"
     ]
    }
   ],
   "source": [
    "dataset = CTUDataset(\"CiteSeer\", data_dir=\"../datasets\", force_remake=False)\n",
    "\n",
    "data = dataset.build_hetero_data(device)\n",
    "\n",
    "n_total = data[dataset.defaults.target_table].y.shape[0]\n",
    "data = T.RandomNodeSplit(split=\"train_rest\", num_val=int(0.30 * n_total), num_test=0)(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jakub/miniconda3/envs/deep-db-learning/lib/python3.10/site-packages/torch/nn/init.py:412: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    }
   ],
   "source": [
    "target = dataset.defaults.target\n",
    "\n",
    "model = Model(\n",
    "    target_table=target[0],\n",
    "    table_col_stats=data.collect(\"col_stats\"),\n",
    "    table_col_names_dict={k: tf.col_names_dict for k, tf in data.collect(\"tf\").items()},\n",
    "    edge_types=data.collect(\"edge_index\").keys(),\n",
    "    embed_dim=64,\n",
    "    num_embedder_layers=4,\n",
    "    num_embedder_heads=4,\n",
    "    embedder_attn_dropout=0.1,\n",
    "    out_dim=dataset.schema[target[0]].columns[target[1]].card,\n",
    ").to(device)\n",
    "lightning_model = LightningModel(model, dataset.defaults.target_table, lr=0.0001).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = NeighborLoader(\n",
    "    data,\n",
    "    num_neighbors=[30] * 5,\n",
    "    batch_size=10000,\n",
    "    input_nodes=(target[0], data[target[0]].train_mask),\n",
    ")\n",
    "\n",
    "val_loader = NeighborLoader(\n",
    "    data,\n",
    "    num_neighbors=[30] * 5,\n",
    "    batch_size=10000,\n",
    "    input_nodes=(target[0], data[target[0]].val_mask),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/jakub/miniconda3/envs/deep-db-learning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:67: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | model       | Model            | 917 K \n",
      "1 | loss_module | CrossEntropyLoss | 0     \n",
      "-------------------------------------------------\n",
      "917 K     Trainable params\n",
      "0         Non-trainable params\n",
      "917 K     Total params\n",
      "3.670     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42fa2699eb91446b9207b5b15253670d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jakub/miniconda3/envs/deep-db-learning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/jakub/miniconda3/envs/deep-db-learning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/jakub/miniconda3/envs/deep-db-learning/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37ee5a0906f540f49b05a55be265e61d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc508089962a4ec08bb2ad24bd09377f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c233eec27a84fb199603758600ff5f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5cc172bf99849b1b40087749dde0ba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b76f4740522c467cb2be1f948dd2f52d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af945c98b995487984e84756b586ffe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "623dfed617a8485481e8ab4bac326b1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c56a1a52915f445ab30e48d50f5af46f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b31c1ea803224f40845e29a4403827da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "237a0bed377c48559e3101744ed71c84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a8ebee026b545d2bee1ed4b372e1288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8b5be40cb0748f9bf31f3f0598683b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f9116a56e5e4e91be74e957e9d65dcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jakub/miniconda3/envs/deep-db-learning/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(\n",
    "    accelerator=device.type,\n",
    "    devices=1,\n",
    "    deterministic=False,\n",
    "    max_epochs=100,\n",
    "    max_steps=-1,\n",
    ")\n",
    "\n",
    "trainer.fit(lightning_model, train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-db-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
