{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from dataclasses import asdict, dataclass, field\n",
    "from typing import Callable, Dict, List, Literal, Optional, Tuple, TypeVar, Union\n",
    "\n",
    "import lightning as L\n",
    "import lightning.pytorch.callbacks as L_callbacks\n",
    "import lovely_tensors as lt\n",
    "\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.entities import Run\n",
    "from mlflow.utils.mlflow_tags import MLFLOW_USER, MLFLOW_PARENT_RUN_ID\n",
    "\n",
    "import optuna\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from simple_parsing import ArgumentParser, DashVariant\n",
    "from sqlalchemy.engine import Connection\n",
    "\n",
    "import torch\n",
    "from torch_geometric.transforms import RandomNodeSplit\n",
    "from torch_geometric.datasets import Entities, Planetoid\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.data.data import EdgeType, NodeType\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "import ray\n",
    "from ray import tune, train\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from ray.air import session as RaySession\n",
    "\n",
    "from db_transformer.data.dataset_defaults.fit_dataset_defaults import FIT_DATASET_DEFAULTS, FITDatasetDefaults, TaskType\n",
    "from db_transformer.data.embedder import CatEmbedder, NumEmbedder\n",
    "from db_transformer.data.embedder.embedders import TableEmbedder\n",
    "from db_transformer.data.fit_dataset import FITRelationalDataset\n",
    "from db_transformer.data.utils import HeteroDataBuilder\n",
    "from db_transformer.helpers.timer import Timer\n",
    "from db_transformer.schema.columns import CategoricalColumnDef, NumericColumnDef\n",
    "from db_transformer.schema.schema import ColumnDef, Schema\n",
    "\n",
    "from models import HeteroGNN\n",
    "from models.layers import NodeApplied, PerFeatureNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataConfig:\n",
    "    pass\n",
    "\n",
    "\n",
    "DatasetType = Literal[\n",
    "    'Accidents', 'Airline', 'Atherosclerosis', 'Basketball_women', 'Bupa', 'Carcinogenesis',\n",
    "    'Chess', 'CiteSeer', 'ConsumerExpenditures', 'CORA', 'CraftBeer', 'Credit', 'cs', 'Dallas', 'DCG', 'Dunur',\n",
    "    'Elti', 'ErgastF1', 'Facebook', 'financial', 'ftp', 'geneea', 'genes', 'Hepatitis_std', 'Hockey', 'imdb_ijs',\n",
    "    'imdb_MovieLens', 'KRK', 'legalActs', 'medical', 'Mondial', 'Mooney_Family', 'MuskSmall', 'mutagenesis',\n",
    "    'nations', 'NBA', 'NCAA', 'Pima', 'PremierLeague', 'PTE', 'PubMed_Diabetes', 'Same_gen', 'SAP', 'SAT',\n",
    "    'Shakespeare', 'Student_loan', 'Toxicology', 'tpcc', 'tpcd', 'tpcds', 'trains', 'university', 'UTube',\n",
    "    'UW_std', 'VisualGenome', 'voc', 'WebKP', 'world'\n",
    "]\n",
    "\n",
    "\n",
    "DEFAULT_DATASET_NAME: DatasetType = 'CORA'\n",
    "\n",
    "\n",
    "AggrType = Literal['sum', 'mean', 'min', 'max', 'cat']\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    dim: int = 64\n",
    "    aggr: AggrType = 'sum'\n",
    "    gnn_layers: List[int] = field(default_factory=lambda: [])\n",
    "    mlp_layers: List[int] = field(default_factory=lambda: [])\n",
    "    batch_norm: bool = False\n",
    "    layer_norm: bool = False\n",
    "    \n",
    "_T = TypeVar('_T')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f1037f00cf0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lt.monkey_patch()\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data(\n",
    "    dataset=DEFAULT_DATASET_NAME,\n",
    "    conn: Optional[Connection] = None,\n",
    "    schema: Optional[Schema] = None,\n",
    "    device=None,\n",
    ") -> _T:\n",
    "    has_sub_connection = conn is None\n",
    "\n",
    "    if has_sub_connection:\n",
    "        conn = FITRelationalDataset.create_remote_connection(dataset)\n",
    "\n",
    "    defaults = FIT_DATASET_DEFAULTS[dataset]\n",
    "\n",
    "    if schema is None:\n",
    "        schema = FITRelationalDataset.create_schema_analyzer(\n",
    "            dataset, conn, verbose=True\n",
    "        ).guess_schema()\n",
    "\n",
    "    builder = HeteroDataBuilder(\n",
    "        conn,\n",
    "        schema,\n",
    "        target_table=defaults.target_table,\n",
    "        target_column=defaults.target_column,\n",
    "        separate_target=True,\n",
    "        create_reverse_edges=True,\n",
    "        fillna_with=0.0,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    data_pd = builder.build_as_pandas()\n",
    "    data = builder.build(with_column_names=True)\n",
    "\n",
    "    if has_sub_connection:\n",
    "        conn.close()\n",
    "\n",
    "    return data, data_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _expand_with_dummy_features(data: HeteroData, column_defs: Dict[NodeType, List[ColumnDef]]):\n",
    "    for node_type in data.node_types:\n",
    "        x = data[node_type].x\n",
    "        if x.shape[-1] == 0:\n",
    "            x = torch.ones((*x.shape[:-1], 1), dtype=x.dtype)\n",
    "            column_defs[node_type] = [NumericColumnDef()]\n",
    "            data[node_type].x = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(\n",
    "    dataset=DEFAULT_DATASET_NAME, data_config: Optional[DataConfig] = None, device=None\n",
    "):\n",
    "    if data_config is None:\n",
    "        data_config = DataConfig()\n",
    "\n",
    "    with FITRelationalDataset.create_remote_connection(dataset) as conn:\n",
    "        defaults = FIT_DATASET_DEFAULTS[dataset]\n",
    "        \n",
    "        print(f'Connected to db {dataset}...')\n",
    "\n",
    "        schema_analyzer = FITRelationalDataset.create_schema_analyzer(\n",
    "            dataset, conn, verbose=True\n",
    "        )\n",
    "        schema = schema_analyzer.guess_schema()\n",
    "        \n",
    "        print('Created a schema...')\n",
    "\n",
    "        (data, column_defs, colnames), data_pd = build_data(\n",
    "            dataset=dataset,\n",
    "            conn=conn,\n",
    "            schema=schema,\n",
    "            # device=device\n",
    "        )\n",
    "        \n",
    "        print('Build the data...')\n",
    "\n",
    "        _expand_with_dummy_features(data, column_defs)\n",
    "\n",
    "        n_total = data[defaults.target_table].x.shape[0]\n",
    "        \n",
    "        data: HeteroData = RandomNodeSplit(\"train_rest\", num_val=int(0.30 * n_total), num_test=0)(data)\n",
    "\n",
    "        return data, data_pd, schema, defaults, column_defs, colnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        schema: Schema,\n",
    "        config: ModelConfig,\n",
    "        edge_types: List[Tuple[str, str, str]],\n",
    "        defaults: FITDatasetDefaults,\n",
    "        column_defs: Dict[str, List[ColumnDef]],\n",
    "        column_names: Dict[str, List[str]],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.defaults = defaults\n",
    "\n",
    "        node_types = list(column_defs.keys())\n",
    "\n",
    "        self.embedder = TableEmbedder(\n",
    "            (CategoricalColumnDef, lambda: CatEmbedder(dim=config.dim)),\n",
    "            (NumericColumnDef, lambda: NumEmbedder(dim=config.dim)),\n",
    "            dim=config.dim,\n",
    "            column_defs=column_defs,\n",
    "            column_names=column_names,\n",
    "        )\n",
    "\n",
    "        node_dims = {k: len(cds) for k, cds in column_defs.items()}\n",
    "\n",
    "        self.layer_norm = (\n",
    "            NodeApplied(\n",
    "                lambda nt: PerFeatureNorm(node_dims[nt], axis=-2), node_types=node_types\n",
    "            )\n",
    "            if config.layer_norm\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        assert defaults.task == TaskType.CLASSIFICATION\n",
    "\n",
    "        out_col_def = schema[defaults.target_table].columns[defaults.target_column]\n",
    "\n",
    "        if not isinstance(out_col_def, CategoricalColumnDef):\n",
    "            raise ValueError()\n",
    "\n",
    "        out_dim = out_col_def.card\n",
    "        gnn_out_dim = config.mlp_layers[0] if config.mlp_layers else out_dim\n",
    "\n",
    "        node_dims = {k: len(cds) * config.dim for k, cds in column_defs.items()}\n",
    "\n",
    "        self.gnn = HeteroGNN(\n",
    "            dims=config.gnn_layers,\n",
    "            node_dims=node_dims,\n",
    "            out_dim=gnn_out_dim,\n",
    "            node_types=node_types,\n",
    "            edge_types=edge_types,\n",
    "            aggr=config.aggr,\n",
    "            batch_norm=config.batch_norm,\n",
    "        )\n",
    "\n",
    "        if config.mlp_layers:\n",
    "            mlp_layer_dims = [*config.mlp_layers, out_dim]\n",
    "\n",
    "            mlp_layers = []\n",
    "\n",
    "            for a, b in zip(mlp_layer_dims[:-1], mlp_layer_dims[1:]):\n",
    "                mlp_layers += [torch.nn.ReLU(), torch.nn.Linear(a, b)]\n",
    "\n",
    "                if config.batch_norm:\n",
    "                    mlp_layers += [torch.nn.BatchNorm1d(b)]\n",
    "\n",
    "            if config.batch_norm:\n",
    "                del mlp_layers[-1]\n",
    "\n",
    "            self.mlp = torch.nn.Sequential(*mlp_layers)\n",
    "        else:\n",
    "            self.mlp = None\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x_dict: Dict[NodeType, torch.Tensor],\n",
    "        edge_dict: Dict[EdgeType, torch.Tensor],\n",
    "    ) -> torch.Tensor:\n",
    "        x_dict = self.embedder(x_dict)\n",
    "\n",
    "        if self.layer_norm is not None:\n",
    "            x_dict = self.layer_norm(x_dict)\n",
    "\n",
    "        x_dict = {k: x.view(*x.shape[:-2], -1) for k, x in x_dict.items()}\n",
    "\n",
    "        x_dict = self.gnn(x_dict, edge_dict)\n",
    "\n",
    "        x = x_dict[self.defaults.target_table]\n",
    "\n",
    "        if self.mlp is not None:\n",
    "            x = self.mlp(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config: tune.TuneConfig):\n",
    "    session = RaySession.get_session()\n",
    "\n",
    "    mlflow_config = config.pop(\"mlflow_config\", None)\n",
    "    client: MlflowClient = mlflow_config[\"client\"]\n",
    "\n",
    "    experiment_name = mlflow_config.pop(\"experiment_name\", None)\n",
    "    experiment_id = client.get_experiment_by_name(experiment_name).experiment_id\n",
    "\n",
    "    parent_run_id = mlflow_config.pop(\"parent_run_id\", None)\n",
    "\n",
    "    run_name: str = mlflow_config.pop(\"run_name\", None)\n",
    "\n",
    "    dataset_name = config[\"dataset\"]\n",
    "\n",
    "    run: Run = client.create_run(\n",
    "        experiment_id,\n",
    "        run_name=run_name + f\"_{session.trial_id}\",\n",
    "        tags={\n",
    "            MLFLOW_USER: \"pelesjak\",\n",
    "            MLFLOW_PARENT_RUN_ID: parent_run_id,\n",
    "            \"Dataset\": dataset_name,\n",
    "            \"trial_id\": session.trial_id,\n",
    "        },\n",
    "    )\n",
    "    run_id = run.info.run_id\n",
    "\n",
    "    params = [mlflow.entities.Param(k, str(v)) for (k, v) in config.items()]\n",
    "    client.log_batch(run_id, params=params)\n",
    "\n",
    "    data, data_pd, schema, defaults, column_defs, colnames = create_data(dataset_name)\n",
    "\n",
    "    model = Model(\n",
    "        schema=schema,\n",
    "        config=ModelConfig(\n",
    "            dim=config[\"embed_dim\"],\n",
    "            aggr=config[\"aggr\"],\n",
    "            gnn_layers=config[\"gnn\"],\n",
    "            mlp_layers=config[\"mlp\"],\n",
    "            batch_norm=config[\"batch_norm\"],\n",
    "            layer_norm=config[\"layer_norm\"],\n",
    "        ),\n",
    "        edge_types=data.edge_types,\n",
    "        defaults=defaults,\n",
    "        column_defs=column_defs,\n",
    "        column_names=colnames,\n",
    "    )\n",
    "\n",
    "    dataloader = DataLoader([data], batch_size=1)\n",
    "\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(True), lr=config[\"lr\"])\n",
    "\n",
    "    target_tbl = data[defaults.target_table]\n",
    "\n",
    "    best_train_acc = 0\n",
    "    best_val_acc = 0\n",
    "\n",
    "    for e in range(config[\"epochs\"]):\n",
    "        model.train()\n",
    "        train_acc = 0\n",
    "        train_norm = 0\n",
    "        for batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            out = model(batch.collect(\"x\"), batch.collect(\"edge_index\"))\n",
    "\n",
    "            mask = target_tbl.train_mask\n",
    "\n",
    "            loss = loss_fn(out[mask], target_tbl.y[mask])\n",
    "            train_acc += (out[mask].argmax(dim=-1) == target_tbl.y[mask]).sum().float()\n",
    "            train_norm += mask.sum()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        val_acc = 0\n",
    "        val_norm = 0\n",
    "        for batch in dataloader:\n",
    "            out = model(batch.collect(\"x\"), batch.collect(\"edge_index\"))\n",
    "            mask = target_tbl.val_mask\n",
    "            val_acc += (out[mask].argmax(dim=-1) == target_tbl.y[mask]).sum().float()\n",
    "            val_norm += mask.sum()\n",
    "\n",
    "        train_acc = (train_acc / train_norm).item()\n",
    "        val_acc = (val_acc / val_norm).item()\n",
    "\n",
    "        if best_train_acc < train_acc:\n",
    "            best_train_acc = train_acc\n",
    "\n",
    "        if best_val_acc < val_acc:\n",
    "            best_val_acc = val_acc\n",
    "\n",
    "        metric_dict = {\n",
    "            \"best_train_acc\": best_train_acc,\n",
    "            \"best_val_acc\": best_val_acc,\n",
    "            \"train_acc\": train_acc,\n",
    "            \"val_acc\": val_acc,\n",
    "        }\n",
    "\n",
    "        timestamp = int(datetime.now().timestamp() * 1000)\n",
    "\n",
    "        metrics = [\n",
    "            mlflow.entities.Metric(key, value, timestamp, e)\n",
    "            for (key, value) in metric_dict.items()\n",
    "        ]\n",
    "        client.log_batch(run_id, metrics=metrics, synchronous=False)\n",
    "\n",
    "        session.report(metric_dict)\n",
    "\n",
    "    client.set_terminated(run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(tracking_uri: str, experiment_name: str, dataset: DatasetType):\n",
    "    mlflow.set_tracking_uri(tracking_uri)\n",
    "    mlflow.set_experiment(experiment_name=experiment_name)\n",
    "    \n",
    "    time_str = datetime.now().strftime(\"%d-%m-%Y,%H:%M:%S\")\n",
    "    run_name = f\"{dataset}_{time_str}\"\n",
    "\n",
    "    with mlflow.start_run(run_name=run_name) as run:\n",
    "        \n",
    "        client = mlflow.tracking.MlflowClient(tracking_uri)\n",
    "        analysis: tune.ExperimentAnalysis = tune.run(\n",
    "            train_model,\n",
    "            metric=\"val_acc\",\n",
    "            mode=\"max\",\n",
    "            search_alg=OptunaSearch(),\n",
    "            num_samples=20,\n",
    "            storage_path=os.getcwd() + \"/results\",\n",
    "            config={\n",
    "                \"lr\": 0.0001,#tune.loguniform(0.00005, 0.001),\n",
    "                \"emed_dim\": [8, 16],\n",
    "                \"aggr\": ['sum', 'mean', 'cat'],\n",
    "                \"gnn\": [[], [16], [16, 8]],\n",
    "                \"mlp\": [[], [16], [16, 8]],\n",
    "                \"batch_norm\": [True, False],\n",
    "                \"layer_norm\": [True, False],\n",
    "                \"dataset\": dataset,\n",
    "                \"epochs\": 10000,\n",
    "                \"mlflow_config\": {\n",
    "                    \"client\": client,\n",
    "                    \"experiment_name\": experiment_name,\n",
    "                    \"run_name\": run_name,\n",
    "                    \"parent_run_id\": run.info.run_id,\n",
    "                },\n",
    "            },\n",
    "        )\n",
    "        best_trial = analysis.best_trial\n",
    "        mlflow.set_tags({MLFLOW_USER: 'pelesjak', 'Dataset': dataset, 'trial_id': best_trial.trial_id})\n",
    "        mlflow.log_params(best_trial.config)\n",
    "        mlflow.log_metrics({k:v for (k,v) in analysis.best_result.items() if k is not None and v is not None and type(v) in [float, int]})\n",
    "        print(f\"Best config: {analysis.best_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-16 12:32:49,263\tINFO tune.py:586 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "[I 2023-11-16 12:32:49,276] A new study created in memory with name: optuna\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-16 12:32:49,277\tWARNING callback.py:137 -- The TensorboardX logger cannot be instantiated because either TensorboardX or one of it's dependencies is not installed. Please make sure you have the latest version of TensorboardX installed: `pip install -U tensorboardx`\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to sample a configuration from OptunaSearch, but no search space has been defined. Either pass the `space` argument when instantiating the search algorithm, or pass a `param_space` to `tune.Tuner()`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/jakub/Documents/ŠKOLA/Ing./Diplomka/deep-db-learning/experiment.ipynb Cell 10\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jakub/Documents/%C5%A0KOLA/Ing./Diplomka/deep-db-learning/experiment.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m experiment_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdeep-db-tests-pelesjak\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jakub/Documents/%C5%A0KOLA/Ing./Diplomka/deep-db-learning/experiment.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m run_experiment(\u001b[39m'\u001b[39;49m\u001b[39mhttp://147.32.83.171:2222\u001b[39;49m\u001b[39m'\u001b[39;49m, experiment_name, \u001b[39m\"\u001b[39;49m\u001b[39mworld\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[1;32m/home/jakub/Documents/ŠKOLA/Ing./Diplomka/deep-db-learning/experiment.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jakub/Documents/%C5%A0KOLA/Ing./Diplomka/deep-db-learning/experiment.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mwith\u001b[39;00m mlflow\u001b[39m.\u001b[39mstart_run(run_name\u001b[39m=\u001b[39mrun_name) \u001b[39mas\u001b[39;00m run:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jakub/Documents/%C5%A0KOLA/Ing./Diplomka/deep-db-learning/experiment.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     client \u001b[39m=\u001b[39m mlflow\u001b[39m.\u001b[39mtracking\u001b[39m.\u001b[39mMlflowClient(tracking_uri)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jakub/Documents/%C5%A0KOLA/Ing./Diplomka/deep-db-learning/experiment.ipynb#X12sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     analysis: tune\u001b[39m.\u001b[39mExperimentAnalysis \u001b[39m=\u001b[39m tune\u001b[39m.\u001b[39;49mrun(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jakub/Documents/%C5%A0KOLA/Ing./Diplomka/deep-db-learning/experiment.ipynb#X12sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         train_model,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jakub/Documents/%C5%A0KOLA/Ing./Diplomka/deep-db-learning/experiment.ipynb#X12sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         metric\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mval_acc\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jakub/Documents/%C5%A0KOLA/Ing./Diplomka/deep-db-learning/experiment.ipynb#X12sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmax\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jakub/Documents/%C5%A0KOLA/Ing./Diplomka/deep-db-learning/experiment.ipynb#X12sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         search_alg\u001b[39m=\u001b[39;49mOptunaSearch(),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jakub/Documents/%C5%A0KOLA/Ing./Diplomka/deep-db-learning/experiment.ipynb#X12sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m         num_samples\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jakub/Documents/%C5%A0KOLA/Ing./Diplomka/deep-db-learning/experiment.ipynb#X12sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m         storage_path\u001b[39m=\u001b[39;49mos\u001b[39m.\u001b[39;49mgetcwd() \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m/results\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jakub/Documents/%C5%A0KOLA/Ing./Diplomka/deep-db-learning/experiment.ipynb#X12sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m         config\u001b[39m=\u001b[39;49m{\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jakub/Documents/%C5%A0KOLA/Ing./Diplomka/deep-db-learning/experiment.ipynb#X12sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m0.0001\u001b[39;49m, \u001b[39m#tune.loguniform(0.00005, 0.001),\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jakub/Documents/%C5%A0KOLA/Ing./Diplomka/deep-db-learning/experiment.ipynb#X12sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39memed_dim\u001b[39;49m\u001b[39m\"\u001b[39;49m: [\u001b[39m8\u001b[39;49m, \u001b[39m16\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jakub/Documents/%C5%A0KOLA/Ing./Diplomka/deep-db-learning/experiment.ipynb#X12sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39maggr\u001b[39;49m\u001b[39m\"\u001b[39;49m: [\u001b[39m'\u001b[39;49m\u001b[39msum\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mmean\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mcat\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jakub/Documents/%C5%A0KOLA/Ing./Diplomka/deep-db-learning/experiment.ipynb#X12sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mgnn\u001b[39;49m\u001b[39m\"\u001b[39;49m: [[], [\u001b[39m16\u001b[39;49m], [\u001b[39m16\u001b[39;49m, \u001b[39m8\u001b[39;49m]],\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jakub/Documents/%C5%A0KOLA/Ing./Diplomka/deep-db-learning/experiment.ipynb#X12sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mmlp\u001b[39;49m\u001b[39m\"\u001b[39;49m: [[], [\u001b[39m16\u001b[39;49m], [\u001b[39m16\u001b[39;49m, \u001b[39m8\u001b[39;49m]],\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jakub/Documents/%C5%A0KOLA/Ing./Diplomka/deep-db-learning/experiment.ipynb#X12sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mbatch_norm\u001b[39;49m\u001b[39m\"\u001b[39;49m: [\u001b[39mTrue\u001b[39;49;00m, \u001b[39mFalse\u001b[39;49;00m],\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jakub/Documents/%C5%A0KOLA/Ing./Diplomka/deep-db-learning/experiment.ipynb#X12sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mlayer_norm\u001b[39;49m\u001b[39m\"\u001b[39;49m: [\u001b[39mTrue\u001b[39;49;00m, \u001b[39mFalse\u001b[39;49;00m],\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jakub/Documents/%C5%A0KOLA/Ing./Diplomka/deep-db-learning/experiment.ipynb#X12sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mdataset\u001b[39;49m\u001b[39m\"\u001b[39;49m: dataset,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jakub/Documents/%C5%A0KOLA/Ing./Diplomka/deep-db-learning/experiment.ipynb#X12sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mepochs\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m10000\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jakub/Documents/%C5%A0KOLA/Ing./Diplomka/deep-db-learning/experiment.ipynb#X12sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mmlflow_config\u001b[39;49m\u001b[39m\"\u001b[39;49m: {\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jakub/Documents/%C5%A0KOLA/Ing./Diplomka/deep-db-learning/experiment.ipynb#X12sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mclient\u001b[39;49m\u001b[39m\"\u001b[39;49m: client,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jakub/Documents/%C5%A0KOLA/Ing./Diplomka/deep-db-learning/experiment.ipynb#X12sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mexperiment_name\u001b[39;49m\u001b[39m\"\u001b[39;49m: experiment_name,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jakub/Documents/%C5%A0KOLA/Ing./Diplomka/deep-db-learning/experiment.ipynb#X12sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mrun_name\u001b[39;49m\u001b[39m\"\u001b[39;49m: run_name,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jakub/Documents/%C5%A0KOLA/Ing./Diplomka/deep-db-learning/experiment.ipynb#X12sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mparent_run_id\u001b[39;49m\u001b[39m\"\u001b[39;49m: run\u001b[39m.\u001b[39;49minfo\u001b[39m.\u001b[39;49mrun_id,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jakub/Documents/%C5%A0KOLA/Ing./Diplomka/deep-db-learning/experiment.ipynb#X12sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m             },\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jakub/Documents/%C5%A0KOLA/Ing./Diplomka/deep-db-learning/experiment.ipynb#X12sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m         },\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jakub/Documents/%C5%A0KOLA/Ing./Diplomka/deep-db-learning/experiment.ipynb#X12sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jakub/Documents/%C5%A0KOLA/Ing./Diplomka/deep-db-learning/experiment.ipynb#X12sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     best_trial \u001b[39m=\u001b[39m analysis\u001b[39m.\u001b[39mbest_trial\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jakub/Documents/%C5%A0KOLA/Ing./Diplomka/deep-db-learning/experiment.ipynb#X12sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     mlflow\u001b[39m.\u001b[39mset_tags({MLFLOW_USER: \u001b[39m'\u001b[39m\u001b[39mpelesjak\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m'\u001b[39m: dataset, \u001b[39m'\u001b[39m\u001b[39mtrial_id\u001b[39m\u001b[39m'\u001b[39m: best_trial\u001b[39m.\u001b[39mtrial_id})\n",
      "File \u001b[0;32m~/miniconda3/envs/deep-db-learning/lib/python3.10/site-packages/ray/tune/tune.py:1007\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, storage_path, storage_filesystem, search_alg, scheduler, checkpoint_config, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, reuse_actors, raise_on_failed_trial, callbacks, max_concurrent_trials, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, chdir_to_trial_dir, local_dir, _experiment_checkpoint_dir, _remote, _remote_string_queue, _entrypoint)\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1006\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m runner\u001b[39m.\u001b[39mis_finished() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m experiment_interrupted_event\u001b[39m.\u001b[39mis_set():\n\u001b[0;32m-> 1007\u001b[0m         runner\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m   1008\u001b[0m         \u001b[39mif\u001b[39;00m has_verbosity(Verbosity\u001b[39m.\u001b[39mV1_EXPERIMENT):\n\u001b[1;32m   1009\u001b[0m             _report_progress(runner, progress_reporter)\n",
      "File \u001b[0;32m~/miniconda3/envs/deep-db-learning/lib/python3.10/site-packages/ray/tune/execution/tune_controller.py:725\u001b[0m, in \u001b[0;36mTuneController.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    720\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callbacks\u001b[39m.\u001b[39mon_step_begin(\n\u001b[1;32m    721\u001b[0m         iteration\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iteration, trials\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_trials\n\u001b[1;32m    722\u001b[0m     )\n\u001b[1;32m    724\u001b[0m \u001b[39m# Ask searcher for more trials\u001b[39;00m\n\u001b[0;32m--> 725\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_update_trial_queue()\n\u001b[1;32m    727\u001b[0m \u001b[39m# Start actors for added trials\u001b[39;00m\n\u001b[1;32m    728\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_add_actors()\n",
      "File \u001b[0;32m~/miniconda3/envs/deep-db-learning/lib/python3.10/site-packages/ray/tune/execution/tune_controller.py:832\u001b[0m, in \u001b[0;36mTuneController._maybe_update_trial_queue\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    827\u001b[0m dont_wait_for_trial \u001b[39m=\u001b[39m (\n\u001b[1;32m    828\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pending_trials \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_running_trials \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_paused_trials\n\u001b[1;32m    829\u001b[0m )\n\u001b[1;32m    831\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pending_trials) \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_pending_trials:\n\u001b[0;32m--> 832\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_trial_queue(blocking\u001b[39m=\u001b[39;49m\u001b[39mnot\u001b[39;49;00m dont_wait_for_trial):\n\u001b[1;32m    833\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    834\u001b[0m     dont_wait_for_trial \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/deep-db-learning/lib/python3.10/site-packages/ray/tune/execution/tune_controller.py:614\u001b[0m, in \u001b[0;36mTuneController._update_trial_queue\u001b[0;34m(self, blocking, timeout)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update_trial_queue\u001b[39m(\u001b[39mself\u001b[39m, blocking: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, timeout: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m600\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[1;32m    602\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Adds next trials to queue if possible.\u001b[39;00m\n\u001b[1;32m    603\u001b[0m \n\u001b[1;32m    604\u001b[0m \u001b[39m    Note that the timeout is currently unexposed to the user.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[39m        Boolean indicating if a new trial was created or not.\u001b[39;00m\n\u001b[1;32m    613\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 614\u001b[0m     trial \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_search_alg\u001b[39m.\u001b[39;49mnext_trial()\n\u001b[1;32m    615\u001b[0m     \u001b[39mif\u001b[39;00m blocking \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m trial:\n\u001b[1;32m    616\u001b[0m         start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/miniconda3/envs/deep-db-learning/lib/python3.10/site-packages/ray/tune/search/search_generator.py:100\u001b[0m, in \u001b[0;36mSearchGenerator.next_trial\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Provides one Trial object to be queued into the TrialRunner.\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \n\u001b[1;32m     96\u001b[0m \u001b[39mReturns:\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[39m    Trial: Returns a single trial.\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_finished():\n\u001b[0;32m--> 100\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_trial_if_possible(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_experiment\u001b[39m.\u001b[39;49mspec)\n\u001b[1;32m    101\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/deep-db-learning/lib/python3.10/site-packages/ray/tune/search/search_generator.py:106\u001b[0m, in \u001b[0;36mSearchGenerator.create_trial_if_possible\u001b[0;34m(self, experiment_spec)\u001b[0m\n\u001b[1;32m    104\u001b[0m logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mcreating trial\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    105\u001b[0m trial_id \u001b[39m=\u001b[39m Trial\u001b[39m.\u001b[39mgenerate_id()\n\u001b[0;32m--> 106\u001b[0m suggested_config \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msearcher\u001b[39m.\u001b[39;49msuggest(trial_id)\n\u001b[1;32m    107\u001b[0m \u001b[39mif\u001b[39;00m suggested_config \u001b[39m==\u001b[39m Searcher\u001b[39m.\u001b[39mFINISHED:\n\u001b[1;32m    108\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_finished \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/deep-db-learning/lib/python3.10/site-packages/ray/tune/search/optuna/optuna_search.py:478\u001b[0m, in \u001b[0;36mOptunaSearch.suggest\u001b[0;34m(self, trial_id)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msuggest\u001b[39m(\u001b[39mself\u001b[39m, trial_id: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[Dict]:\n\u001b[1;32m    477\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_space:\n\u001b[0;32m--> 478\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    479\u001b[0m             UNDEFINED_SEARCH_SPACE\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    480\u001b[0m                 \u001b[39mcls\u001b[39m\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, space\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mspace\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    481\u001b[0m             )\n\u001b[1;32m    482\u001b[0m         )\n\u001b[1;32m    483\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metric \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mode:\n\u001b[1;32m    484\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    485\u001b[0m             UNDEFINED_METRIC_MODE\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    486\u001b[0m                 \u001b[39mcls\u001b[39m\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, metric\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metric, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mode\n\u001b[1;32m    487\u001b[0m             )\n\u001b[1;32m    488\u001b[0m         )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to sample a configuration from OptunaSearch, but no search space has been defined. Either pass the `space` argument when instantiating the search algorithm, or pass a `param_space` to `tune.Tuner()`."
     ]
    }
   ],
   "source": [
    "experiment_name = \"deep-db-tests-pelesjak\"\n",
    "\n",
    "run_experiment('http://147.32.83.171:2222', experiment_name, \"world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-db-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
