{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from dataclasses import asdict, dataclass, field\n",
    "from typing import Callable, Dict, List, Literal, Optional, Tuple, TypeVar, Union, get_args\n",
    "\n",
    "# import lightning as L\n",
    "# import lightning.pytorch.callbacks as L_callbacks\n",
    "import lovely_tensors as lt\n",
    "\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.entities import Run\n",
    "from mlflow.utils.mlflow_tags import MLFLOW_USER, MLFLOW_PARENT_RUN_ID\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from simple_parsing import ArgumentParser, DashVariant\n",
    "from sqlalchemy.engine import Connection\n",
    "\n",
    "import torch\n",
    "from torch_geometric.transforms import RandomNodeSplit\n",
    "from torch_geometric.datasets import Entities, Planetoid\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.data.data import EdgeType, NodeType\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from ray.air import session as RaySession\n",
    "\n",
    "from db_transformer.data.dataset_defaults.fit_dataset_defaults import FIT_DATASET_DEFAULTS, FITDatasetDefaults, TaskType\n",
    "from db_transformer.data.embedder import CatEmbedder, NumEmbedder\n",
    "from db_transformer.data.embedder.embedders import TableEmbedder\n",
    "from db_transformer.data.fit_dataset import FITRelationalDataset\n",
    "from db_transformer.data.utils import HeteroDataBuilder\n",
    "from db_transformer.helpers.timer import Timer\n",
    "from db_transformer.schema.columns import CategoricalColumnDef, NumericColumnDef\n",
    "from db_transformer.schema.schema import ColumnDef, Schema\n",
    "\n",
    "from models import HeteroGNN\n",
    "from models.layers import NodeApplied, PerFeatureNorm\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataConfig:\n",
    "    pass\n",
    "\n",
    "\n",
    "DatasetType = Literal[\n",
    "    'Accidents', 'Airline', 'Atherosclerosis', 'Basketball_women', 'Bupa', 'Carcinogenesis',\n",
    "    'Chess', 'CiteSeer', 'ConsumerExpenditures', 'CORA', 'CraftBeer', 'Credit', 'cs', 'Dallas', 'DCG', 'Dunur',\n",
    "    'Elti', 'ErgastF1', 'Facebook', 'financial', 'ftp', 'geneea', 'genes', 'Hepatitis_std', 'Hockey', 'imdb_ijs',\n",
    "    'imdb_MovieLens', 'KRK', 'legalActs', 'medical', 'Mondial', 'Mooney_Family', 'MuskSmall', 'mutagenesis',\n",
    "    'nations', 'NBA', 'NCAA', 'Pima', 'PremierLeague', 'PTE', 'PubMed_Diabetes', 'Same_gen', 'SAP', 'SAT',\n",
    "    'Shakespeare', 'Student_loan', 'Toxicology', 'tpcc', 'tpcd', 'tpcds', 'trains', 'university', 'UTube',\n",
    "    'UW_std', 'VisualGenome', 'voc', 'WebKP', 'world'\n",
    "]\n",
    "\n",
    "\n",
    "DEFAULT_DATASET_NAME: DatasetType = 'CORA'\n",
    "\n",
    "\n",
    "AggrType = Literal['sum', 'mean', 'min', 'max', 'cat']\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    dim: int = 64\n",
    "    aggr: AggrType = 'sum'\n",
    "    gnn_layers: List[int] = field(default_factory=lambda: [])\n",
    "    mlp_layers: List[int] = field(default_factory=lambda: [])\n",
    "    batch_norm: bool = False\n",
    "    layer_norm: bool = False\n",
    "    \n",
    "_T = TypeVar('_T')\n",
    "\n",
    "DEFAULT_EXPERIMENT_NAME = \"deep-db-tests-pelesjak\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--num_samples'], dest='num_samples', nargs=None, const=None, default=5, type=<class 'int'>, choices=None, required=False, help=None, metavar=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = ArgumentParser()\n",
    "parser.add_argument(\"--dataset\", type=str, default=DEFAULT_DATASET_NAME, choices=get_args(DatasetType))\n",
    "parser.add_argument(\"--experiment\", type=str, default=DEFAULT_EXPERIMENT_NAME)\n",
    "parser.add_argument(\"--num_samples\", type=int, default=5)\n",
    "parser.add_argument(\"--cuda\", default=False, action='store_true')\n",
    "parser.add_argument(\"--run_name\", type=str, default=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f4c4ff413f0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lt.monkey_patch()\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data(\n",
    "    dataset=DEFAULT_DATASET_NAME,\n",
    "    conn: Optional[Connection] = None,\n",
    "    schema: Optional[Schema] = None,\n",
    "    device=None,\n",
    ") -> _T:\n",
    "    has_sub_connection = conn is None\n",
    "\n",
    "    if has_sub_connection:\n",
    "        conn = FITRelationalDataset.create_remote_connection(dataset, dialect='postgresql')\n",
    "\n",
    "    defaults = FIT_DATASET_DEFAULTS[dataset]\n",
    "\n",
    "    if schema is None:\n",
    "        schema = FITRelationalDataset.create_schema_analyzer(\n",
    "            dataset, conn, verbose=True\n",
    "        ).guess_schema()\n",
    "\n",
    "    builder = HeteroDataBuilder(\n",
    "        conn,\n",
    "        schema,\n",
    "        target_table=defaults.target_table,\n",
    "        target_column=defaults.target_column,\n",
    "        separate_target=True,\n",
    "        create_reverse_edges=True,\n",
    "        fillna_with=0.0,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    data_pd = builder.build_as_pandas()\n",
    "    data = builder.build(with_column_names=True)\n",
    "\n",
    "    if has_sub_connection:\n",
    "        conn.close()\n",
    "\n",
    "    return data, data_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _expand_with_dummy_features(data: HeteroData, column_defs: Dict[NodeType, List[ColumnDef]]):\n",
    "    for node_type in data.node_types:\n",
    "        x = data[node_type].x\n",
    "        if x.shape[-1] == 0:\n",
    "            x = torch.ones((*x.shape[:-1], 1), dtype=x.dtype)\n",
    "            column_defs[node_type] = [NumericColumnDef()]\n",
    "            data[node_type].x = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(\n",
    "    dataset=DEFAULT_DATASET_NAME, data_config: Optional[DataConfig] = None, device=None\n",
    "):\n",
    "    if data_config is None:\n",
    "        data_config = DataConfig()\n",
    "        \n",
    "    usePostgres = True\n",
    "    \n",
    "    FITDatasetDefaults.case_sensitive = not usePostgres\n",
    "\n",
    "    with FITRelationalDataset.create_remote_connection(dataset, dialect=('postgresql' if usePostgres else 'mariadb')) as conn:    \n",
    "        defaults = FIT_DATASET_DEFAULTS[dataset]\n",
    "        \n",
    "        print(f'Connected to db {dataset}...')\n",
    "\n",
    "        schema_analyzer = FITRelationalDataset.create_schema_analyzer(\n",
    "            dataset, conn, verbose=True\n",
    "        )\n",
    "        schema = schema_analyzer.guess_schema()\n",
    "        \n",
    "        print('Created a schema...')\n",
    "\n",
    "        (data, column_defs, colnames), data_pd = build_data(\n",
    "            dataset=dataset,\n",
    "            conn=conn,\n",
    "            schema=schema,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        print('Build the data...')\n",
    "\n",
    "        _expand_with_dummy_features(data, column_defs)\n",
    "\n",
    "        n_total = data[defaults.target_table].x.shape[0]\n",
    "        \n",
    "        data: HeteroData = RandomNodeSplit(\"train_rest\", num_val=int(0.30 * n_total), num_test=0)(data)\n",
    "\n",
    "        return data, data_pd, schema, defaults, column_defs, colnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        schema: Schema,\n",
    "        config: ModelConfig,\n",
    "        edge_types: List[Tuple[str, str, str]],\n",
    "        defaults: FITDatasetDefaults,\n",
    "        column_defs: Dict[str, List[ColumnDef]],\n",
    "        column_names: Dict[str, List[str]],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.defaults = defaults\n",
    "\n",
    "        node_types = list(column_defs.keys())\n",
    "\n",
    "        self.embedder = TableEmbedder(\n",
    "            (CategoricalColumnDef, lambda: CatEmbedder(dim=config.dim)),\n",
    "            (NumericColumnDef, lambda: NumEmbedder(dim=config.dim)),\n",
    "            dim=config.dim,\n",
    "            column_defs=column_defs,\n",
    "            column_names=column_names,\n",
    "        )\n",
    "\n",
    "        node_dims = {k: len(cds) for k, cds in column_defs.items()}\n",
    "\n",
    "        self.layer_norm = (\n",
    "            NodeApplied(\n",
    "                lambda nt: PerFeatureNorm(node_dims[nt], axis=-2), node_types=node_types\n",
    "            )\n",
    "            if config.layer_norm\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        assert defaults.task == TaskType.CLASSIFICATION\n",
    "\n",
    "        out_col_def = schema[defaults.target_table].columns[defaults.target_column]\n",
    "\n",
    "        if not isinstance(out_col_def, CategoricalColumnDef):\n",
    "            raise ValueError()\n",
    "\n",
    "        out_dim = out_col_def.card\n",
    "        gnn_out_dim = config.mlp_layers[0] if config.mlp_layers else out_dim\n",
    "\n",
    "        node_dims = {k: len(cds) * config.dim for k, cds in column_defs.items()}\n",
    "\n",
    "        self.gnn = HeteroGNN(\n",
    "            dims=config.gnn_layers,\n",
    "            node_dims=node_dims,\n",
    "            out_dim=gnn_out_dim,\n",
    "            node_types=node_types,\n",
    "            edge_types=edge_types,\n",
    "            aggr=config.aggr,\n",
    "            batch_norm=config.batch_norm,\n",
    "        )\n",
    "\n",
    "        if config.mlp_layers:\n",
    "            mlp_layer_dims = [*config.mlp_layers, out_dim]\n",
    "\n",
    "            mlp_layers = []\n",
    "\n",
    "            for a, b in zip(mlp_layer_dims[:-1], mlp_layer_dims[1:]):\n",
    "                mlp_layers += [torch.nn.ReLU(), torch.nn.Linear(a, b)]\n",
    "\n",
    "                if config.batch_norm:\n",
    "                    mlp_layers += [torch.nn.BatchNorm1d(b)]\n",
    "\n",
    "            if config.batch_norm:\n",
    "                del mlp_layers[-1]\n",
    "\n",
    "            self.mlp = torch.nn.Sequential(*mlp_layers)\n",
    "        else:\n",
    "            self.mlp = None\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x_dict: Dict[NodeType, torch.Tensor],\n",
    "        edge_dict: Dict[EdgeType, torch.Tensor],\n",
    "    ) -> torch.Tensor:\n",
    "        x_dict = self.embedder(x_dict)\n",
    "\n",
    "        if self.layer_norm is not None:\n",
    "            x_dict = self.layer_norm(x_dict)\n",
    "\n",
    "        x_dict = {k: x.view(*x.shape[:-2], -1) for k, x in x_dict.items()}\n",
    "\n",
    "        x_dict = self.gnn(x_dict, edge_dict)\n",
    "\n",
    "        x = x_dict[self.defaults.target_table]\n",
    "\n",
    "        if self.mlp is not None:\n",
    "            x = self.mlp(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_run(config: tune.TuneConfig):\n",
    "  session = RaySession.get_session()\n",
    "  assert session != None\n",
    "\n",
    "  mlflow_config = config.pop(\"mlflow_config\", None)\n",
    "  client: MlflowClient = mlflow_config[\"client\"]\n",
    "\n",
    "  experiment_name = mlflow_config.pop(\"experiment_name\", None)\n",
    "  experiment_id = client.get_experiment_by_name(experiment_name).experiment_id\n",
    "\n",
    "  parent_run_id = mlflow_config.pop(\"parent_run_id\", None)\n",
    "\n",
    "  run_name: str = mlflow_config.pop(\"run_name\", None)\n",
    "\n",
    "  run = client.create_run(\n",
    "      experiment_id,\n",
    "      run_name=run_name + f\"_{session.trial_id}\",\n",
    "      tags={\n",
    "          MLFLOW_USER: \"pelesjak\",\n",
    "          MLFLOW_PARENT_RUN_ID: parent_run_id,\n",
    "          \"Dataset\": config[\"dataset\"],\n",
    "          \"trial_id\": session.trial_id,\n",
    "      },\n",
    "  )\n",
    "  return session, client, run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config: tune.TuneConfig):\n",
    "    \n",
    "    print(f'Cuda available: {torch.cuda.is_available()}')\n",
    "    session, client, run = prepare_run(config)\n",
    "    \n",
    "    run_id = run.info.run_id\n",
    "\n",
    "    params = [mlflow.entities.Param(k, str(v)) for (k, v) in config.items()]\n",
    "    client.log_batch(run_id, params=params)\n",
    "    \n",
    "    try:\n",
    "        device = config.pop(\"device\", \"cpu\") if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        data, data_pd, schema, defaults, column_defs, colnames = create_data(config[\"dataset\"], device=device)\n",
    "\n",
    "        model = Model(\n",
    "            schema=schema,\n",
    "            config=ModelConfig(\n",
    "                dim=config[\"embed_dim\"],\n",
    "                aggr=config[\"aggr\"],\n",
    "                gnn_layers=config[\"gnn\"],\n",
    "                mlp_layers=config[\"mlp\"],\n",
    "                batch_norm=config[\"batch_norm\"],\n",
    "                layer_norm=config[\"layer_norm\"],\n",
    "            ),\n",
    "            edge_types=data.edge_types,\n",
    "            defaults=defaults,\n",
    "            column_defs=column_defs,\n",
    "            column_names=colnames,\n",
    "        )\n",
    "        \n",
    "        model = model.to(device)\n",
    "\n",
    "        dataloader = DataLoader([data], batch_size=1)\n",
    "\n",
    "        loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(True), lr=config[\"lr\"], betas=config['betas'])\n",
    "\n",
    "        target_tbl = data[defaults.target_table]\n",
    "\n",
    "        best_train_acc = 0\n",
    "        best_val_acc = 0\n",
    "\n",
    "        for e in range(config[\"epochs\"]):\n",
    "            model.train()\n",
    "            train_acc = 0\n",
    "            train_norm = 0\n",
    "            for batch in dataloader:\n",
    "                batch = batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                out = model(batch.collect(\"x\"), batch.collect(\"edge_index\"))\n",
    "\n",
    "                mask = target_tbl.train_mask\n",
    "\n",
    "                loss = loss_fn(out[mask], target_tbl.y[mask])\n",
    "                train_acc += (out[mask].argmax(dim=-1) == target_tbl.y[mask]).sum().float()\n",
    "                train_norm += mask.sum()\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            model.eval()\n",
    "            val_acc = 0\n",
    "            val_norm = 0\n",
    "            for batch in dataloader:\n",
    "                batch = batch.to(device)\n",
    "                out = model(batch.collect(\"x\"), batch.collect(\"edge_index\"))\n",
    "                mask = target_tbl.val_mask\n",
    "                val_acc += (out[mask].argmax(dim=-1) == target_tbl.y[mask]).sum().float()\n",
    "                val_norm += mask.sum()\n",
    "\n",
    "            train_acc = (train_acc / train_norm).item()\n",
    "            val_acc = (val_acc / val_norm).item()\n",
    "\n",
    "            if best_train_acc < train_acc:\n",
    "                best_train_acc = train_acc\n",
    "\n",
    "            if best_val_acc < val_acc:\n",
    "                best_val_acc = val_acc\n",
    "\n",
    "            metric_dict = {\n",
    "                \"best_train_acc\": best_train_acc,\n",
    "                \"best_val_acc\": best_val_acc,\n",
    "                \"train_acc\": train_acc,\n",
    "                \"val_acc\": val_acc,\n",
    "            }\n",
    "            \n",
    "            # print(f'val_acc: {val_acc}, best_val_acc: {best_val_acc}', end='\\r')\n",
    "\n",
    "            timestamp = int(datetime.now().timestamp() * 1000)\n",
    "\n",
    "            metrics = [\n",
    "                mlflow.entities.Metric(key, value, timestamp, e)\n",
    "                for (key, value) in metric_dict.items()\n",
    "            ]\n",
    "            client.log_batch(run_id, metrics=metrics, synchronous=False)\n",
    "\n",
    "            session.report(metric_dict)\n",
    "\n",
    "        client.set_terminated(run_id)\n",
    "        \n",
    "    except Exception as ex:\n",
    "        client.set_tag(run_id, 'exception', str(ex))\n",
    "        raise ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(tracking_uri: str, experiment_name: str, dataset: DatasetType, num_samples: int, useCuda = False, run_name: str = None):\n",
    "    mlflow.set_tracking_uri(tracking_uri)\n",
    "    mlflow.set_experiment(experiment_name=experiment_name)\n",
    "    \n",
    "    time_str = datetime.now().strftime(\"%d-%m-%Y,%H:%M:%S\")\n",
    "    run_name = f\"{dataset}_{time_str}\" if run_name is None else run_name\n",
    "\n",
    "\n",
    "    with mlflow.start_run(run_name=run_name) as run:\n",
    "        \n",
    "        client = mlflow.tracking.MlflowClient(tracking_uri)\n",
    "        \n",
    "        analysis: tune.ExperimentAnalysis = tune.run(\n",
    "            train_model,\n",
    "            verbose=1,\n",
    "            metric=\"best_val_acc\",\n",
    "            mode=\"max\",\n",
    "            search_alg=OptunaSearch(),\n",
    "            num_samples=num_samples,\n",
    "            storage_path=os.getcwd() + \"/results\",\n",
    "            resources_per_trial={\"gpu\": 1, \"cpu\": 2} if useCuda else None,\n",
    "            config={\n",
    "                \"lr\": 0.0001,#tune.loguniform(0.00005, 0.001),\n",
    "                \"betas\": [0.99, 0.9995],\n",
    "                \"embed_dim\": tune.choice([8, 16]),\n",
    "                \"aggr\": tune.choice(['sum', 'mean']),\n",
    "                \"gnn\": tune.choice([[], [16], [16, 8]]),\n",
    "                \"mlp\": tune.choice([[], [16], [16, 8]]),\n",
    "                \"batch_norm\": tune.choice([True, False]),\n",
    "                \"layer_norm\": tune.choice([False]),\n",
    "                \"dataset\": dataset,\n",
    "                \"epochs\": 1000,\n",
    "                \"device\": \"cuda\" if useCuda else \"cpu\",\n",
    "                \"mlflow_config\": {\n",
    "                    \"client\": client,\n",
    "                    \"experiment_name\": experiment_name,\n",
    "                    \"run_name\": run_name,\n",
    "                    \"parent_run_id\": run.info.run_id,\n",
    "                },\n",
    "            },\n",
    "        )\n",
    "        best_trial = analysis.best_trial\n",
    "        mlflow.set_tags({MLFLOW_USER: 'pelesjak', 'Dataset': dataset, 'trial_id': best_trial.trial_id})\n",
    "        mlflow.log_params(best_trial.config)\n",
    "        mlflow.log_metrics({k:v for (k,v) in analysis.best_result.items() if k is not None and v is not None and type(v) in [float, int]})\n",
    "        print(f\"Best config: {analysis.best_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment('http://147.32.83.171:2222', DEFAULT_EXPERIMENT_NAME, \"world\", 1, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args()\n",
    "\n",
    "run_experiment('http://147.32.83.171:2222', args.experiment, args.dataset, args.num_samples, args.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-db-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
